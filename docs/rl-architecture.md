# RL架构说明：训练在后端，推理在前端

## 架构原则

**核心原则**：
- ✅ **训练逻辑**：完全在Python后端实现
- ✅ **推理逻辑**：前端使用训练好的模型进行推理
- ✅ **经验收集**：前端收集游戏经验，发送到后端
- ✅ **模型管理**：后端负责模型训练、保存和加载

## 架构图

```
┌─────────────────────────────────────────────────────────┐
│                    Frontend (Next.js)                    │
├─────────────────────────────────────────────────────────┤
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐ │
│  │ Game Engine  │  │ RL Inference │  │ Experience   │ │
│  │ - 游戏逻辑    │  │ - 调用后端API│  │ Collector    │ │
│  │ - 渲染        │  │ - 获取动作   │  │ - 收集经验   │ │
│  │ - 用户输入    │  │ - 不训练     │  │ - 发送到后端 │ │
│  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘ │
│         │                 │                  │         │
│         └─────────────────┴──────────────────┴─────────┘
│                          │ HTTP
│                          │ JSON
└──────────────────────────┼──────────────────────────────┘
                           │
┌──────────────────────────▼──────────────────────────────┐
│                  Backend (Python/FastAPI)                │
├─────────────────────────────────────────────────────────┤
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐ │
│  │ API Layer    │  │ RL Trainer   │  │ Model Store │ │
│  │ - /predict   │  │ - DQN/PPO    │  │ - 保存模型   │ │
│  │ - /train     │  │ - 训练循环    │  │ - 加载模型   │ │
│  │ - /experience│  │ - 更新权重   │  │              │ │
│  └──────┬───────┘  └──────┬───────┘  └──────────────┘ │
│         │                 │                            │
│         └─────────────────┴────────────────────────────┘
│                          │
│  ┌───────────────────────▼────────────────────────────┐ │
│  │              Replay Buffer                          │ │
│  │  - 存储经验数据                                     │ │
│  │  - 批量训练                                        │ │
│  └─────────────────────────────────────────────────────┘ │
└──────────────────────────────────────────────────────────┘
```

## 数据流

### 1. 经验收集流程

```
前端游戏运行
  ↓
收集经验 (state, action, reward, nextState, done)
  ↓
经验收集器缓冲（每50条）
  ↓
POST /api/experience → 后端
  ↓
后端存储到经验回放缓冲区
```

### 2. 训练流程（后端）

```
后端训练服务
  ↓
从经验回放缓冲区采样
  ↓
DQN/PPO训练
  ↓
更新模型权重
  ↓
保存模型到文件系统
```

### 3. 推理流程（前端）

```
前端提取当前状态
  ↓
POST /api/predict → 后端
  ↓
后端加载模型并推理
  ↓
返回动作
  ↓
前端执行动作
```

## 代码组织

### 前端代码（只负责推理）

#### `lib/api/rlAgent.ts`
- **职责**：调用后端API进行推理
- **不包含**：训练逻辑、Q表更新

#### `lib/api/experienceCollector.ts`
- **职责**：收集经验并批量发送到后端
- **不包含**：训练逻辑

#### `hooks/useRLInference.ts`
- **职责**：管理推理状态，调用后端API
- **不包含**：训练逻辑、Q-Learning实现

#### `lib/rl/` 目录
- **保留**：`stateExtractor.ts` - 状态提取（前后端都需要）
- **保留**：`rewardCalculator.ts` - 奖励计算（前后端都需要）
- **移除训练逻辑**：`qLearning.ts` 可以保留作为参考，但不应该在前端使用

### 后端代码（负责训练）

#### `app/services/rl/`
- `replay_buffer.py` - 经验回放缓冲区 ✅
- `dqn.py` - DQN实现（待实现）
- `trainer.py` - 训练器（待实现）
- `model_manager.py` - 模型管理（待实现）

#### `app/api/routes.py`
- `POST /api/experience` - 接收经验 ✅
- `POST /api/predict` - 模型推理（待完善）
- `POST /api/train` - 开始训练（待实现）
- `GET /api/train/status` - 训练状态 ✅

## 当前状态

### ✅ 已完成

1. **前端**：
   - ✅ 经验收集器（`experienceCollector.ts`）
   - ✅ RL推理Agent（`rlAgent.ts`）
   - ✅ 推理Hook（`useRLInference.ts`）
   - ✅ 状态提取和奖励计算（供前后端使用）

2. **后端**：
   - ✅ 经验回放缓冲区（`replay_buffer.py`）
   - ✅ API接口定义（`routes.py`）
   - ✅ 数据模型（`models/`）

### ⬜ 待实现

1. **后端训练逻辑**：
   - ⬜ DQN实现（`app/services/rl/dqn.py`）
   - ⬜ 训练器（`app/services/rl/trainer.py`）
   - ⬜ 模型保存和加载
   - ⬜ 训练循环集成到API

2. **后端推理**：
   - ⬜ 模型加载逻辑
   - ⬜ 实际推理实现（当前是占位符）

## 注意事项

### ❌ 前端不应该做的

- ❌ 实现Q-Learning训练逻辑
- ❌ 更新Q表
- ❌ 管理训练状态
- ❌ 保存模型权重

### ✅ 前端应该做的

- ✅ 调用后端API进行推理
- ✅ 收集游戏经验
- ✅ 发送经验到后端
- ✅ 显示推理结果和统计

### ✅ 后端应该做的

- ✅ 实现所有RL算法（DQN、PPO等）
- ✅ 管理训练循环
- ✅ 保存和加载模型
- ✅ 提供推理API

## 迁移指南

如果你之前使用了前端的训练逻辑，需要：

1. **移除前端训练代码**：
   - 删除或注释掉 `useRLTraining.ts` 中的训练逻辑
   - 移除 `qLearning.ts` 的使用

2. **使用推理模式**：
   - 使用 `useRLInference` 替代 `useRLTraining`
   - 使用 `RLAgent` 调用后端API

3. **确保后端运行**：
   - 后端必须运行才能进行推理
   - 如果后端不可用，会回退到简单规则

## 下一步

1. ⬜ 实现后端DQN训练
2. ⬜ 实现模型保存和加载
3. ⬜ 完善推理API
4. ⬜ 添加训练监控和可视化

